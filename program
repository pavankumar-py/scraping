from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import sqlite3
import time
from urllib.parse import urljoin

def create_database():
    """Create SQLite database and table"""
    conn = sqlite3.connect('funding_opportunities.db')
    cursor = conn.cursor()
    
    # Create table exactly as requested
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS funding_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT,
            type TEXT,
            source TEXT,
            p_content TEXT,
            span_content TEXT
        )
    ''')
    
    conn.commit()
    return conn

def setup_driver():
    """Setup Chrome driver"""
    print("Setting up Chrome driver...")
    
    chrome_options = Options()
    # Remove headless to see what's happening
    # chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--window-size=1920,1080')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    return driver

def scrape_main_page():
    """Scrape only the main page for views-row classes 1-35"""
    url = "https://www.indiascienceandtechnology.gov.in/funding-opportunities/startups"
    driver = None
    
    try:
        driver = setup_driver()
        print(f"Loading: {url}")
        
        driver.get(url)
        
        # Wait for page to load completely
        print("Waiting for page to load...")
        time.sleep(10)  # Give more time for JS to load
        
        # Try to wait for specific content
        try:
            WebDriverWait(driver, 30).until(
                EC.presence_of_element_located((By.CLASS_NAME, "views-row"))
            )
            print("Found views-row elements!")
        except:
            print("Timeout waiting for views-row elements, but continuing...")
        
        # Get page source
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # Debug: Print all classes that contain 'views-row'
        all_views = soup.find_all('div', class_=lambda x: x and 'views-row' in str(x))
        print(f"\nFound {len(all_views)} elements with 'views-row' in class name:")
        
        for i, elem in enumerate(all_views[:10]):  # Show first 10
            print(f"Element {i+1}: classes = {elem.get('class')}")
            h4 = elem.find('h4')
            if h4:
                print(f"  H4 text: {h4.get_text(strip=True)[:50]}...")
        
        scraped_data = []
        
        # Try to scrape views-row-1 through views-row-35
        for i in range(1, 36):
            # Try different class name patterns
            class_patterns = [
                f"views-row views-row-{i}",
                f"views-row-{i}",
            ]
            
            row_element = None
            for pattern in class_patterns:
                # Try exact match
                row_element = soup.find('div', class_=pattern)
                if row_element:
                    print(f"Found row {i} with pattern: {pattern}")
                    break
                
                # Try partial match
                row_element = soup.find('div', class_=lambda x: x and pattern in ' '.join(x) if x else False)
                if row_element:
                    print(f"Found row {i} with partial match: {pattern}")
                    break
            
            if row_element:
                # Extract h4 and its link
                h4_element = row_element.find('h4')
                name = ""
                source = ""
                
                if h4_element:
                    name = h4_element.get_text(strip=True)
                    link = h4_element.find('a')
                    if link and link.get('href'):
                        source = urljoin(url, link.get('href'))
                
                # Extract p content
                p_element = row_element.find('p')
                p_content = ""
                span_content = ""
                
                if p_element:
                    p_content = p_element.get_text(strip=True)
                    
                    # Extract span separately
                    span_element = p_element.find('span')
                    if span_element:
                        span_content = span_element.get_text(strip=True)
                
                # Add to data
                scraped_data.append({
                    'name': name,
                    'type': 'grant',
                    'source': source,
                    'p_content': p_content,
                    'span_content': span_content
                })
                
                print(f"Row {i}: {name[:30]}... | {source[:50]}...")
            else:
                print(f"Row {i}: NOT FOUND")
        
        print(f"\nTotal scraped: {len(scraped_data)} items")
        return scraped_data
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return []
        
    finally:
        if driver:
            input("Press Enter to close browser...")  # Keep browser open to see what's loaded
            driver.quit()

def save_to_database(data, conn):
    """Save data to database"""
    cursor = conn.cursor()
    
    # Clear existing data
    cursor.execute('DELETE FROM funding_data')
    
    # Insert new data
    for item in data:
        cursor.execute('''
            INSERT INTO funding_data (name, type, source, p_content, span_content)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            item['name'],
            item['type'], 
            item['source'],
            item['p_content'],
            item['span_content']
        ))
    
    conn.commit()
    print(f"Saved {len(data)} records to database")

def show_database_content(conn):
    """Show what's in the database"""
    cursor = conn.cursor()
    cursor.execute('SELECT * FROM funding_data')
    rows = cursor.fetchall()
    
    print(f"\n=== DATABASE CONTENT ({len(rows)} records) ===")
    for row in rows:
        print(f"ID: {row[0]}")
        print(f"Name: {row[1]}")
        print(f"Type: {row[2]}")
        print(f"Source: {row[3]}")
        print(f"P Content: {row[4][:100]}...")
        print(f"Span Content: {row[5]}")
        print("-" * 50)

def main():
    """Main function"""
    print("=== SIMPLE FUNDING SCRAPER ===")
    print("Target: views-row views-row-1 to views-row views-row-35")
    print("Fields: h4 content, href link, p content, span content")
    
    conn = create_database()
    
    try:
        # Scrape main page
        data = scrape_main_page()
        
        if data:
            # Save to database
            save_to_database(data, conn)
            
            # Show results
            show_database_content(conn)
            
            print(f"\n✅ SUCCESS: Scraped {len(data)} items")
            print("Database: funding_opportunities.db")
        else:
            print("\n❌ NO DATA SCRAPED")
            print("Check if the website structure has changed")
            
    except Exception as e:
        print(f"Error in main: {e}")
        
    finally:
        conn.close()

if __name__ == "__main__":
    main()
